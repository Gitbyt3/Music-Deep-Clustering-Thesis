{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca12266",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "594ef22c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchaudio.prototype.pipelines import VGGISH\n",
    "import openl3\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb5fef4",
   "metadata": {},
   "source": [
    "# MVP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb65668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split(audios, sr, segments=3):\n",
    "    \"\"\"\n",
    "    Loads in the mp3 files available for the model and splits each into x segments. Default is 3 so each segment will be 5 seconds long (15 second total each)\n",
    "\n",
    "    Args:\n",
    "        audios -> Dictionary of audio ids and full audio path\n",
    "                    e.g. {0:'...7horsemethlabzosostickerthewolfofwallstreetsample1mp3.mp3'}\n",
    "        sr -> Desired sample rate for loading in the audio files with\n",
    "                Audio files are natively 44.1k but not set as default to enable use in other functions\n",
    "\n",
    "    Outputs:\n",
    "        1. Audio IDs -> Numpy array, duplicates as ID will be copied in for each segment (use for finding the corresponding film metadata)\n",
    "        2. Librosa loaded audio samples -> 2D Numpy array, each row corresponds to a unique audio segment\n",
    "    \"\"\"\n",
    "\n",
    "    ids, data = [], []\n",
    "\n",
    "    for id, path in tqdm(audios.items(), desc=\"Processing audio files\"):\n",
    "        signal, _ = librosa.load(path, sr=sr)\n",
    "        segment_length = len(signal) / segments\n",
    "\n",
    "        # Librosa loaded file is just a numpy array so possible to segment using slicing\n",
    "        for i in range(segments):\n",
    "            start = int(i * segment_length)\n",
    "            end = int(start + segment_length)\n",
    "            samples = signal[start:end]\n",
    "            ids.append(id), data.append(samples)\n",
    "\n",
    "    return np.array(ids), np.array(data)\n",
    "\n",
    "def feature_extraction(data, sr, hop_length=1024, n_fft=4096):\n",
    "    \"\"\"\n",
    "    Extracts audio features from the librosa time series and standardises\n",
    "\n",
    "    Args:\n",
    "        data -> Numpy array of Librosa loaded audio segments\n",
    "        sr -> Sample rate\n",
    "        hop_length & n_fft -> Hyperparameters to be shared across various features\n",
    "\n",
    "    Output:\n",
    "        X -> Numpy matrix of audio features of shape (# samples, # features)\n",
    "    \"\"\"\n",
    "\n",
    "    X = []\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        for i in tqdm(range(len(data)), desc=\"Extracting audio features\"):\n",
    "            y = data[i]\n",
    "            \n",
    "            # --- Timbral Features ---\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, n_mels=60, n_fft=n_fft, hop_length=hop_length, fmin=20, fmax=22050, lifter=30, win_length=n_fft, window='hann')\n",
    "            f_mfccmean = np.mean(mfccs, axis=1)\n",
    "            f_mfccstd = np.std(mfccs, axis=1)\n",
    "            f_speccentoid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft))\n",
    "            f_specband = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft))\n",
    "            f_speccontrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft))\n",
    "\n",
    "            # --- Rhythm Features ---\n",
    "            f_tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, hop_length=hop_length)\n",
    "            beat_times = librosa.frames_to_time(beat_frames, sr=sr, hop_length=hop_length, n_fft=n_fft)\n",
    "            beat_indices = librosa.time_to_frames(beat_times, sr=sr, hop_length=hop_length, n_fft=n_fft)\n",
    "            f_beatrms = np.nan_to_num(np.mean(librosa.feature.rms(y=y, hop_length=hop_length)[0, beat_indices])) # Error handling -> Convert to 0 as tempo will also be 0\n",
    "            f_onsetstrength = np.mean(librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft))\n",
    "            \n",
    "            # --- Loudness Features ---\n",
    "            rms = librosa.feature.rms(y=y, hop_length=hop_length)\n",
    "            f_rmsmean = np.mean(rms)\n",
    "            f_rmsstd = np.std(rms)\n",
    "\n",
    "            # --- Harmonic Features ---\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft)\n",
    "            f_chromamean = np.mean(chroma, axis=1)\n",
    "            f_chromastd = np.std(chroma, axis=1) \n",
    "\n",
    "            features = np.hstack((f_mfccmean, f_mfccstd, f_speccentoid, f_specband, f_speccontrast, f_tempo, f_beatrms, f_onsetstrength, f_rmsmean, f_rmsstd, f_chromamean, f_chromastd))\n",
    "            X.append(features)\n",
    "\n",
    "    return np.array(X)\n",
    "\n",
    "def feature_selection_standardised(X, kpca, standardise=True, var_threshold=0.15, corr_threshold=0.75, n_components=8):\n",
    "    \"\"\"\n",
    "    Performs variance and correlation feature selection and kernel PCA on the feature matrix\n",
    "    Drops columns with a variance less than var_threshold and drops one of two correlated features greater than corr_threshold\n",
    "    Also standardises the data, very important for PCA\n",
    "\n",
    "    Args:\n",
    "        X -> Feature matrix (# samples, # features)\n",
    "        kpca -> Boolean, performs PCA if true\n",
    "        standardise -> Boolean, standardises feature matrix if true\n",
    "        var_threshold -> Lower limit of acceptable variance, features with lower variance will be dropped\n",
    "        corr_threshold -> Upper limit of acceptable correlation, only drops one of the two correlated features if correlation is greater than the threshold\n",
    "        n_components -> Number of components for PCA\n",
    "\n",
    "    Output:\n",
    "        X_reduced -> Matrix with reduced features\n",
    "    \"\"\"\n",
    "\n",
    "    X_reduced = VarianceThreshold(threshold=var_threshold).fit_transform(X)\n",
    "\n",
    "    df = pd.DataFrame(X_reduced)\n",
    "    corrs = df.corr().abs()         # Only absolute size of correlation matters, not direction\n",
    "\n",
    "    # 1. Returns boolean matrix of same shape as correlation matrix. True for the values above the self correlation diagonal\n",
    "    # 2. Transforms the correlation matrix -> All NaN below and including the diagonal, keeps values above the diagonal\n",
    "    upper = corrs.where(np.triu(np.ones(corrs.shape), k=1).astype(bool))\n",
    "    X_reduced = df.drop(columns=[col for col in upper.columns if any(upper[col] > corr_threshold)])     # Finds columns in correlation matrix with any correlation higher than the threshold\n",
    "    \n",
    "    if standardise: X_reduced = StandardScaler().fit_transform(X_reduced)\n",
    "\n",
    "    if kpca:\n",
    "        kpca = KernelPCA(n_components=n_components)\n",
    "        X_reduced = kpca.fit_transform(X_reduced)\n",
    "        for i, l in enumerate(kpca.eigenvalues_): print(f'Component {i} roughly explains {l / sum(kpca.eigenvalues_):.3%} variance')\n",
    "\n",
    "    return X_reduced\n",
    "\n",
    "def evaluate_plot(X, model, k=30):\n",
    "    \"\"\"\n",
    "    Evaluates multiple unsupervised metrics against the number of clusters\n",
    "    \n",
    "    Args:\n",
    "        X -> Standardised feature matrix\n",
    "        model -> Model to use, either GMM or kMeans\n",
    "        k -> Max clusters to test\n",
    "    \n",
    "    Outputs:\n",
    "        Plots of metric v cluster\n",
    "    \"\"\"\n",
    "\n",
    "    clusters, CH, SS, DB = [], [], [], []\n",
    "\n",
    "    if model == 'kmeans':\n",
    "        for k in range(2, k):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "            labels = kmeans.labels_\n",
    "            clusters.append(k), CH.append(calinski_harabasz_score(X, labels)), SS.append(silhouette_score(X, labels)), DB.append(davies_bouldin_score(X, labels))\n",
    "\n",
    "    elif model == 'gmm':\n",
    "        for k in range(2, k):\n",
    "            gmm = GaussianMixture(n_components=k, random_state=42).fit(X)\n",
    "            labels = gmm.predict(X)\n",
    "            clusters.append(k), CH.append(calinski_harabasz_score(X, labels)), SS.append(silhouette_score(X, labels)), DB.append(davies_bouldin_score(X, labels))\n",
    "\n",
    "    metrics = [\n",
    "        (CH, 'Calinski-Harabasz Score', 'Calinski-Harabasz v Number of Clusters'),\n",
    "        (SS, 'Silhouette Score', 'Silhouette Score v Number of Clusters'),\n",
    "        (DB, 'Davies-Bouldin Score', 'Davies-Bouldin v Number of Clusters')]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    for i, (metric, label, title) in enumerate(metrics):\n",
    "        ax[i].plot(clusters, metric)\n",
    "        ax[i].set_ylabel(label, fontweight='bold')\n",
    "        ax[i].set_xlabel('Clusters', fontweight='bold')\n",
    "        ax[i].set_title(title, fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def cluster_viz(X, model, clusters):\n",
    "    \"\"\"\n",
    "    Visualises cluster assignment for a feature matrix of just 2 features\n",
    "    Use PCA to reduce feature matrix to 2 components\n",
    "\n",
    "    Args:\n",
    "        X -> Standardised feature matrix of 2 features\n",
    "        model -> Clustering model to use, k-means or gmm\n",
    "        clusters -> Number of clusters to use in model\n",
    "\n",
    "    Output:\n",
    "        Plot of colour-coded clusters\n",
    "    \"\"\"\n",
    "\n",
    "    if model == 'kmeans':\n",
    "        labels = KMeans(n_clusters=clusters, random_state=42).fit_predict(X)\n",
    "    elif model == 'gmm':\n",
    "        labels = GaussianMixture(n_components=clusters, random_state=42).fit_predict(X)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(9, 6))\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "    ax.set_title(f'{model}\\nk = {clusters}', fontweight='bold', fontsize=17)\n",
    "    ax.legend(*scatter.legend_elements(), title='Clusters')\n",
    "    plt.show()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def cluster_predict(X, clusters, sample_audio_ids, audio_dict, AV_scores):\n",
    "    \"\"\"\n",
    "    Clusters the feature matrix using k-Means and aggregates samples from the same audio file together for each label.\n",
    "    For example, assume 2 samples from audio ID 54 are labelled as cluster 3. The median of these two samples will be calculated and stored with the process repeated for each cluster\n",
    "\n",
    "    Aggregates (median) arousal-valence scores for each unique audio file, given that participants scored the same audio file multiple times and multiple participants scored the same audio file\n",
    "    Aggregates familiarity ratings according to the highest count\n",
    "\n",
    "    Args:\n",
    "        X -> Standardised feature matrix\n",
    "        clusters -> Number of clusters to use in k-means\n",
    "        sample_audio_ids -> List of audio IDs for each sample in the feature matrix\n",
    "        audio_dict -> Dictionary of audio IDs to filepaths --- {0:'...7horsemethlabzosostickerthewolfofwallstreetsample1mp3.mp3'}\n",
    "        AV_scores -> Dataframe of arousal-valence scores and film metadata\n",
    "\n",
    "    Outputs:\n",
    "        1. cluster_df -> Dataframe of filepaths, clusters, aggregate arousal + valence, and film metadata\n",
    "        2. median_cluster_scores -> Dataframe of median arousal + valence for each cluster\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- Clustering + Retrieving distance to assigned cluster -----\n",
    "    kmeans = KMeans(n_clusters=clusters, random_state=42).fit(X)\n",
    "    dist, label = kmeans.transform(X), kmeans.predict(X)\n",
    "    dist_assigned = dist[np.arange(X.shape[0]), label]\n",
    "\n",
    "    # ----- Aggregate Distance for Samples from same Audio in same Cluster -----\n",
    "    dist_df = pd.DataFrame({'file':sample_audio_ids, 'label':label, 'dist_centroid':dist_assigned})     \\\n",
    "                .groupby(['file','label'])['dist_centroid'].median().reset_index()                      \\\n",
    "                .sort_values(by=['label','dist_centroid']).reset_index(drop=True)\n",
    "    dist_df['file'] = [os.path.basename(audio_dict[id]) for id in dist_df['file']]\n",
    "\n",
    "    # ----- Aggregate Valence, Arousal, Familiarity + Join with AV_scores for Film Metadata -----\n",
    "    agg_AV = AV_scores.groupby('file').agg({'valence':'median','arousal':'median','familiarity':lambda x: x.value_counts().index[0]})\n",
    "    agg_AV = pd.merge(agg_AV, AV_scores.drop(columns=['familiarity','arousal','valence']).drop_duplicates(subset=['file']).set_index('file'), on='file').reset_index()\n",
    "\n",
    "    # ----- Merge Distance with Aggregate AV + AV Aggregation for Clusters -----\n",
    "    cluster_df = pd.merge(dist_df, agg_AV, how='left', on='file')\n",
    "    median_cluster_scores = cluster_df.groupby('label').agg({'valence':'median', 'arousal':'median'}).reset_index()\n",
    "\n",
    "    return cluster_df, median_cluster_scores\n",
    "\n",
    "def find_recommendations(cluster_df, cluster_scores, top_k=5):\n",
    "    \"\"\"\n",
    "    Requests an input from the user for an arousal-valence score\n",
    "    Calculates Euclidean distances between input and AV scores of each of the centroids - Returns cluster label\n",
    "    Uses cluster label to filter clustered samples for only the relevant cluster, sorts by distance to centroid and returns the top_k audio tracks with the shortest distances\n",
    "\n",
    "    Args:\n",
    "        cluster_df -> Dataframe of clustered audio tracks including their AV scores, distance to assigned centroid and label\n",
    "        cluster_scores -> Median AV scores of all the audio tracks assigned to each cluster\n",
    "        top_k -> k results to retrieve\n",
    "\n",
    "    Output:\n",
    "        retrieved_tracks -> Dataframe of audio tracks assigned to the closest centroid to the input Arousal-Valence score. Length of dataframe = top_k\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- Arousal Input -----\n",
    "    while True:\n",
    "        try:\n",
    "            arousal = float(input('Please input an arousal score between -1 and +1'))\n",
    "            if abs(arousal) > 1:\n",
    "                print('Please enter a number between -1 and +1')\n",
    "                continue\n",
    "            break\n",
    "        except:\n",
    "            print('Please enter a valid number')\n",
    "            continue\n",
    "\n",
    "    # ----- Valence Input -----\n",
    "    while True:\n",
    "        try:\n",
    "            valence = float(input('Please input an valence score between -1 and +1'))\n",
    "            if abs(valence) > 1:\n",
    "                print('Please enter a number between -1 and +1')\n",
    "                continue\n",
    "            break\n",
    "        except:\n",
    "            print('Please enter a valid number')\n",
    "            continue\n",
    "\n",
    "    \n",
    "    input_score = np.array([arousal, valence])\n",
    "    centroids = cluster_scores[['arousal','valence']].to_numpy()\n",
    "    dist = np.linalg.norm(centroids - input_score, axis=1)              # Euclidean distance between input AV score and AV scores of each of the centroids\n",
    "    closest_centroid = cluster_scores.iloc[np.argmin(dist)]['label']    # Finds index of minimum distance and uses this to filter cluster_scores\n",
    "    \n",
    "    retrieved_tracks = cluster_df.loc[cluster_df['label'] == closest_centroid].sort_values(by='dist_centroid', ignore_index=True).head(top_k)\n",
    "    print(f'Arousal-Valence: ({str(input_score[0])}, {str(input_score[1])})')\n",
    "\n",
    "    return retrieved_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ba71c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AV_scores = pd.read_csv(os.path.join(os.getcwd(), '..', 'CSV_files', 'arousal_valence.csv'))\n",
    "# audio_dict = {index:os.path.join(os.getcwd(), '..', 'audios', file) for index, file in enumerate(sorted(AV_scores['file'].unique()))}\n",
    "\n",
    "# ids, data  = load_and_split(audio_dict, sr=44100)\n",
    "# X = feature_extraction(data, sr=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4763852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_processed = feature_selection_standardised(X, kpca=True, n_components=2)\n",
    "# cluster_viz(X_processed, model='kmeans', clusters=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e65f16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_processed = feature_selection_standardised(X, kpca=True, n_components=6)\n",
    "# evaluate_plot(X_processed, 'kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4498a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_processed = feature_selection_standardised(X, kpca=True, n_components=6)\n",
    "# cluster_df, cluster_scores = cluster_predict(X_processed, 7, ids, audio_dict, AV_scores)\n",
    "\n",
    "# cluster_df.to_csv(os.path.join(os.getcwd(), '..', 'CSV_files', 'tracks_clustered.csv'), index=False)\n",
    "# cluster_scores.to_csv(os.path.join(os.getcwd(), '..', 'CSV_files', 'cluster_scores.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68f63b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_recommendations(cluster_df, cluster_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf11359",
   "metadata": {},
   "source": [
    "# MVP 2\n",
    "\n",
    "Clustering component:\n",
    "1. Extract embeddings using pretrained models (VGGish and Openl3)\n",
    "2. Feed embeddings into a Variational Deep Embedding -> Integrate a VAE and GMM for clustering\n",
    "\n",
    "Auxiliary variable component:\n",
    "Use cross-modal retrieval with contrastive learning (similar to CLIP)\n",
    "1. Encode features and encode arousal-valence so that they're on the same space\n",
    "2. Train the model to push similar features together and dissimilar features apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32046a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            prev_dim = h_dim\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        self.mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.logvar = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.encoder(X)\n",
    "        mu, logvar = self.mu(X), self.logvar(X)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mu + std * torch.randn_like(std)\n",
    "        return mu, logvar, z\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for h_dim in hidden_dims[::-1]:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            prev_dim = h_dim\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "        self.reconstructed = nn.Linear(hidden_dims[0], input_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.decoder(z)\n",
    "        return self.reconstructed(z)\n",
    "\n",
    "class AVencoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(AVencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, 64),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(64, output_dim))\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, av):\n",
    "        av = self.encoder(av)\n",
    "        return self.norm(av)\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.projector = nn.Sequential(nn.Linear(input_dim, output_dim, bias=False),\n",
    "                                       nn.BatchNorm1d(output_dim),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(output_dim, output_dim, bias=False),\n",
    "                                       nn.BatchNorm1d(output_dim, affine=False))\n",
    "    def forward(self, x):\n",
    "        return self.projector(x)\n",
    "\n",
    "class VaDE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, n_clusters):\n",
    "        super(VaDE, self).__init__()\n",
    "        self.input_dim, self.hidden_dims, self.latent_dim, self.n_clusters = input_dim, hidden_dims, latent_dim, n_clusters\n",
    "\n",
    "        self.encoder = Encoder(input_dim, hidden_dims, latent_dim)\n",
    "        self.decoder = Decoder(input_dim, hidden_dims, latent_dim)\n",
    "\n",
    "        self.cluster_weights = nn.Parameter(torch.ones(n_clusters)/n_clusters)\n",
    "        self.cluster_means = nn.Parameter(torch.randn(n_clusters, latent_dim) * 0.05)\n",
    "        self.cluster_logvars = nn.Parameter(torch.randn(n_clusters, latent_dim) * 0.05 - 1)\n",
    "\n",
    "        self.av_encoder = AVencoder(input_dim=2, output_dim=latent_dim)\n",
    "        self.projector = ProjectionHead(input_dim=latent_dim, output_dim=latent_dim)\n",
    "\n",
    "    def forward(self, X, AV):\n",
    "        mu, logvar, z = self.encoder(X)\n",
    "        recon_X = self.decoder(z)\n",
    "        av_encoded = self.av_encoder(AV)\n",
    "        return mu, logvar, z, recon_X, av_encoded\n",
    "    \n",
    "    def cluster_probabilities(self, z):\n",
    "        cluster_weights = torch.softmax(self.cluster_weights, dim=0)\n",
    "        cluster_logvars = self.cluster_logvars.clamp(min=-10, max=10)\n",
    "        cluster_vars = torch.exp(cluster_logvars) + 1e-8\n",
    "\n",
    "        z = z.unsqueeze(1).expand(-1, self.n_clusters, -1)\n",
    "\n",
    "        log_2pi = np.log(2 * np.pi)\n",
    "        squared_diff = (z - self.cluster_means).pow(2)\n",
    "        log_prob_z_given_c = -0.5 * (torch.sum(cluster_logvars + squared_diff / cluster_vars, dim=2) + self.latent_dim * log_2pi)\n",
    "        log_prob_c_given_z = torch.log(cluster_weights.unsqueeze(0) + 1e-8) + log_prob_z_given_c\n",
    "        gamma = torch.softmax(log_prob_c_given_z, dim=1)\n",
    "        return gamma\n",
    "\n",
    "    def loss(self, X, recon_X, mu, logvar, gamma, current_epoch):\n",
    "        recon_loss = F.mse_loss(recon_X, X, reduction='sum') / (X.size(0) + 1e-10)\n",
    "        \n",
    "        logvar = logvar.clamp(min=-15, max=15)\n",
    "        cluster_logvars = self.cluster_logvars.clamp(min=-15, max=15)\n",
    "        \n",
    "        var = torch.exp(logvar) + 1e-10\n",
    "        cluster_vars = torch.exp(cluster_logvars) + 1e-10\n",
    "        \n",
    "        mu = mu.unsqueeze(1)\n",
    "        var = var.unsqueeze(1)\n",
    "        \n",
    "        kl_z_per_cluster = 0.5 * (\n",
    "            cluster_logvars.unsqueeze(0) - logvar.unsqueeze(1) + \n",
    "            (var + (mu - self.cluster_means.unsqueeze(0)).pow(2)) / cluster_vars.unsqueeze(0) - 1\n",
    "        )\n",
    "        \n",
    "        kl_z = torch.sum(gamma.unsqueeze(-1) * kl_z_per_cluster, dim=[1,2]).mean()\n",
    "        \n",
    "        log_gamma = torch.log(gamma + 1e-20)\n",
    "        cluster_weights = torch.softmax(self.cluster_weights, dim=0)\n",
    "        kl_c = torch.sum(gamma * (log_gamma - torch.log(cluster_weights.unsqueeze(0)) + 1e-20), dim=1).mean()\n",
    "        \n",
    "        kl_weight = min(1.0, current_epoch/150 * 1.0)\n",
    "        total_loss = recon_loss + kl_weight * (kl_z + kl_c)\n",
    "        \n",
    "        return total_loss, recon_loss, kl_z, kl_c\n",
    "\n",
    "    def contrastive_loss(self, z_projected, av_projected, cluster_labels, temperature=0.1):\n",
    "        z_projected = z_projected / (z_projected.norm(dim=1, keepdim=True) + 1e-10)\n",
    "        av_projected = av_projected / (av_projected.norm(dim=1, keepdim=True) + 1e-10)\n",
    "        \n",
    "        sim_matrix = torch.mm(z_projected, av_projected.t()) / temperature\n",
    "        \n",
    "        pos_mask = (cluster_labels.unsqueeze(1) == cluster_labels.unsqueeze(0)).float()\n",
    "        pos_mask.fill_diagonal_(0)\n",
    "        \n",
    "        max_sim = sim_matrix.max(dim=1, keepdim=True)[0].detach()\n",
    "        exp_sim = torch.exp(sim_matrix - max_sim)\n",
    "        \n",
    "        pos_term = (exp_sim * pos_mask).sum(dim=1)\n",
    "        neg_term = (exp_sim * (1 - pos_mask)).sum(dim=1)\n",
    "        \n",
    "        loss = -(torch.log(pos_term + 1e-10) - torch.log(neg_term + 1e-10)).mean()\n",
    "        return loss\n",
    "\n",
    "    def predict_cluster(self, x):\n",
    "        with torch.no_grad():\n",
    "            _, _, z = self.encoder(x)\n",
    "            soft_clusters = self.cluster_probabilities(z)\n",
    "            return z, torch.argmax(soft_clusters, dim=1)\n",
    "\n",
    "def embedding_extractor(device, shuffle=True):\n",
    "    AV_scores = pd.read_csv(os.path.join(os.getcwd(), '..', 'CSV_files', 'arousal_valence.csv'))\n",
    "    audios = {index:os.path.join(os.getcwd(), '..', 'audios', file) for index, file in enumerate(sorted(AV_scores['file'].unique()))}\n",
    "\n",
    "    if os.path.exists('audio_embeddings.pt'):\n",
    "        embeddings = torch.load('audio_embeddings.pt', map_location='cpu')\n",
    "        frames_per_audio = int(embeddings.shape[0] / len(audios))\n",
    "        ids = [id for id in audios for _ in range(frames_per_audio)]\n",
    "\n",
    "    else:\n",
    "        vggish_processer, vggish_model = VGGISH.get_input_processor(), VGGISH.get_model().to(device)\n",
    "        vggish_model.eval()\n",
    "\n",
    "        ids, vgg_, openl3_ = [], [], []\n",
    "\n",
    "        for id, path in tqdm(audios.items(), desc=\"Extracting audio embeddings...\"):\n",
    "\n",
    "            vgg_data, vgg_sr = torchaudio.load(path)\n",
    "            if vgg_sr != 16000: vgg_data = torchaudio.functional.resample(vgg_data, vgg_sr, 16000)\n",
    "            if vgg_data.shape[0] > 1: vgg_data = vgg_data.mean(dim=0, keepdim=True)\n",
    "            vgg_data = vgg_data.flatten()\n",
    "            with torch.no_grad():\n",
    "                vggish_embeddings = vggish_model(vggish_processer(vgg_data))\n",
    "\n",
    "            openl3_data, openl3_sr = torchaudio.load(path)\n",
    "            if openl3_data.shape[0] > 1: openl3_data.mean(dim=0, keepdim=True)\n",
    "            openl3_data = openl3_data.flatten().numpy()\n",
    "            target_hop_size = (len(openl3_data) / openl3_sr) / vggish_embeddings.shape[0]\n",
    "            openl3_embeddings, _ = openl3.get_audio_embedding(openl3_data, openl3_sr, content_type=\"music\", embedding_size=512, hop_size=target_hop_size, center=False)\n",
    "            \n",
    "            min_length = min(vggish_embeddings.shape[0], openl3_embeddings.shape[0])\n",
    "            vggish_embeddings = vggish_embeddings[:min_length]\n",
    "            openl3_embeddings = openl3_embeddings[:min_length]\n",
    "\n",
    "            for _ in range(min_length): ids.append(id)\n",
    "            vgg_.append(vggish_embeddings), openl3_.append(openl3_embeddings)\n",
    "\n",
    "        vgg_, openl3_ = torch.cat(vgg_, dim=0), torch.from_numpy(np.concatenate(openl3_, axis=0))\n",
    "        embeddings = torch.cat((vgg_, openl3_), dim=1)\n",
    "        torch.save(embeddings, 'audio_embeddings.pt')\n",
    "\n",
    "    AV_scores_filtered = AV_scores.drop(index=AV_scores[(AV_scores['valence'] == 0.0) & (AV_scores['arousal'] == 0.0)].index)\n",
    "    AV_scores_filtered = AV_scores_filtered[['file','arousal','valence']].groupby(['file']).median()\n",
    "\n",
    "    AV = []\n",
    "    for id in ids:\n",
    "        filename = os.path.basename(audios[id])\n",
    "        AV.append(AV_scores_filtered.loc[AV_scores_filtered.index == filename].values)\n",
    "    AV = torch.tensor(np.concatenate(AV))\n",
    "\n",
    "    embeddings, AV = torch.from_numpy(MinMaxScaler().fit_transform(embeddings.numpy())).float().to(device), torch.from_numpy(MinMaxScaler(feature_range=(-1, 1)).fit_transform(AV.numpy())).float().to(device)\n",
    "\n",
    "    dataset = TensorDataset(embeddings, AV)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=shuffle)\n",
    "\n",
    "    return AV_scores, audios, ids, embeddings, dataloader\n",
    "\n",
    "def train_model(embs, dataloader, hidden_dims, latent_dim, n_clusters, lr, epochs, device, display_clusters=False):\n",
    "    model = VaDE(embs.shape[1], hidden_dims=hidden_dims, latent_dim=latent_dim, n_clusters=n_clusters).to(device)\n",
    "    with torch.no_grad():\n",
    "        _, _, initial_z = model.encoder(embs)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=77).fit(initial_z.cpu().numpy())\n",
    "        model.cluster_means.data = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)\n",
    "\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser, max_lr=lr*10, epochs=epochs, steps_per_epoch=len(dataloader), pct_start=0.3)\n",
    "\n",
    "    train_RL, train_KLz, train_KLc, train_VL, train_CL, train_TL = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_RL, epoch_KLz, epoch_KLc, epoch_VL, epoch_CL, epoch_TL = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        epoch_clusters = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            X, av = batch[0], batch[1]\n",
    "\n",
    "            mu, logvar, z, recon_X, av_encoded = model(X, av)\n",
    "            gamma = model.cluster_probabilities(z)\n",
    "            VL, RL, KL_z, KL_c = model.loss(X, recon_X, mu, logvar, gamma, epoch)\n",
    "            \n",
    "            cluster_labels = torch.argmax(gamma, dim=1)\n",
    "            audio_projections = model.projector(z)\n",
    "            av_projections = model.projector(av_encoded)\n",
    "            CL = model.contrastive_loss(audio_projections, av_projections, cluster_labels)\n",
    "\n",
    "            TL = VL + CL\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            TL.backward()\n",
    "            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "            optimiser.step()\n",
    "\n",
    "            epoch_RL += RL.item()\n",
    "            epoch_KLz += KL_z.item()\n",
    "            epoch_KLc += KL_c.item()\n",
    "            epoch_VL += VL.item()\n",
    "            epoch_CL += CL.item()\n",
    "            epoch_TL += TL.item()\n",
    "            epoch_clusters.append(cluster_labels)\n",
    "\n",
    "        epoch_RL /= len(dataloader)\n",
    "        epoch_KLz /= len(dataloader)\n",
    "        epoch_KLc /= len(dataloader)\n",
    "        epoch_VL /= len(dataloader)\n",
    "        epoch_CL /= len(dataloader)\n",
    "        epoch_TL /= len(dataloader)\n",
    "\n",
    "        train_RL.append(epoch_RL), train_KLz.append(epoch_KLz), train_KLc.append(epoch_KLc), train_VL.append(epoch_VL), train_CL.append(epoch_CL), train_TL.append(epoch_TL)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}\\nReconstruction Loss: {epoch_RL:.1f}. Cluster Divergence: {epoch_KLc:.1f}. Distribution Divergence: {epoch_KLz:.1f}. VaDE Loss: {epoch_VL:.1f}\\nContrastive Loss: {epoch_CL:.1f}\\nTotal Loss: {epoch_TL:.1f}\\n')\n",
    "\n",
    "        if display_clusters:\n",
    "            print(f\"{torch.bincount(torch.cat(epoch_clusters))}\\n\")\n",
    "\n",
    "    all_z, all_clusters = [], []\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        X = batch[0].to(device)\n",
    "        z, clusters = model.predict_cluster(X)\n",
    "        all_z.append(z.cpu().numpy()), all_clusters.append(clusters.cpu().numpy())\n",
    "    all_z, all_clusters = np.concatenate(all_z), np.concatenate(all_clusters)\n",
    "\n",
    "    return model, all_z, all_clusters, train_RL, train_KLz, train_KLc, train_VL, train_CL, train_TL\n",
    "\n",
    "def precompute_audio_projections(model, dataloader):\n",
    "    database = {'projected_audio': [], 'cluster_ids': []}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for embs, _ in dataloader:\n",
    "            z, clusters = model.predict_cluster(embs)\n",
    "            proj = model.projector(z)\n",
    "            database['projected_audio'].append(proj), database['cluster_ids'].append(clusters)\n",
    "\n",
    "    database['projected_audio'] = torch.cat(database['projected_audio'])\n",
    "    database['cluster_ids'] = torch.cat(database['cluster_ids'])\n",
    "\n",
    "    return database\n",
    "\n",
    "def retrieve_k_closest(AV_query, database, ids, audio_dict, AV_scores, model, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        query_tensor = torch.tensor(AV_query, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "        query_projected = model.projector(model.av_encoder(query_tensor))\n",
    "\n",
    "    query_projected = F.normalize(query_projected, p=2, dim=1)\n",
    "    database['projected_audio'] = F.normalize(database['projected_audio'], p=2, dim=1)\n",
    "\n",
    "    similarities = torch.mm(query_projected, database['projected_audio'].T)\n",
    "    topk_similarity, topk_indices = torch.topk(similarities.squeeze(), k=20)\n",
    "    topk_similarity = topk_similarity.tolist()\n",
    "    topk_clusters = database['cluster_ids'][topk_indices].tolist()\n",
    "    topk_ids = [ids[index] for index in topk_indices.tolist()]\n",
    "\n",
    "    unique_ids, unique_similarities, unique_clusters = [], [], []\n",
    "    for sim, id, cluster in zip(topk_similarity, topk_ids, topk_clusters):\n",
    "        if id not in unique_ids:\n",
    "            unique_ids.append(id), unique_similarities.append(sim), unique_clusters.append(cluster)\n",
    "            if len(unique_ids) == 10:\n",
    "                break\n",
    "\n",
    "    unique_filenames = [os.path.basename(audio_dict[id]) for id in unique_ids]\n",
    "    AV_scores_deduped = AV_scores.drop_duplicates(subset='file')[['file','title','composer','film','director','genre']]\n",
    "\n",
    "    results_df = pd.DataFrame({'file':unique_filenames, 'similarity':unique_similarities, 'cluster_id':unique_clusters}).merge(AV_scores_deduped, how='left', on='file')\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AV_scores, audio_dict, ids, embs, dataloader = embedding_extractor(device)\n",
    "\n",
    "hidden_dims = [512, 256, 128]\n",
    "latent_dim = 32\n",
    "n_clusters = 6\n",
    "lr = 0.00001\n",
    "epochs = 200\n",
    "\n",
    "model, latent_space, clusters, train_RL, train_KLz, train_KLc, train_VL, train_CL, train_TL = train_model(  embs=embs, dataloader=dataloader, hidden_dims=hidden_dims, latent_dim=latent_dim,\n",
    "                                                                                                            n_clusters=n_clusters, lr=lr, epochs=epochs, device=device, display_clusters=False)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "z_reduced = tsne.fit_transform(latent_space)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "ax[0].scatter(z_reduced[:, 0], z_reduced[:, 1], c=clusters, cmap='tab10', alpha=0.7, s=10)\n",
    "ax[0].set_title(\"t-SNE Visualisation of Latent Space\")\n",
    "ax[0].set_xlabel(\"t-SNE Dimension 1\")\n",
    "ax[0].set_ylabel(\"t-SNE Dimension 2\")\n",
    "\n",
    "epoch_range = range(1, epochs + 1)\n",
    "ax[1].plot(epoch_range, train_RL, label='Reconstruction Loss')\n",
    "ax[1].plot(epoch_range, train_KLz, label='Distribution Divergence')\n",
    "ax[1].plot(epoch_range, train_KLc, label='Cluster Divergence')\n",
    "ax[1].plot(epoch_range, train_VL, label='VaDE Loss')\n",
    "ax[1].plot(epoch_range, train_CL, label='Contrastive Loss')\n",
    "ax[1].plot(epoch_range, train_TL, label='Total Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Loss Components over Training Loop')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ace6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SS = []\n",
    "# for k in range(2, 11):\n",
    "#     model, latent_space, clusters, _, _, _, _, _, _ = train_model(embs=embs,\n",
    "#                                                                   dataloader=dataloader,\n",
    "#                                                                   hidden_dims=hidden_dims,\n",
    "#                                                                   latent_dim=latent_dim,\n",
    "#                                                                   n_clusters=k,\n",
    "#                                                                   lr=lr,\n",
    "#                                                                   epochs=epochs,\n",
    "#                                                                   device=device,\n",
    "#                                                                   display_clusters=False)\n",
    "\n",
    "#     if len(np.unique(clusters)) > 1:\n",
    "#         SS.append(silhouette_score(latent_space, clusters, metric='cosine'))\n",
    "#     else:\n",
    "#         SS.append(-1)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# ax.plot(SS)\n",
    "# ax.set_xlabel('Number of clusters')\n",
    "# ax.set_xticklabels([f\"{tick + 2}\" for tick in ax.get_xticks()])\n",
    "# ax.set_ylabel('Silhouette Score')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35debbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AV_scores, audio_dict, ids, embs, dataloader = embedding_extractor(device, shuffle=False)\n",
    "\n",
    "database = precompute_audio_projections(model, dataloader)\n",
    "\n",
    "test = retrieve_k_closest((-0.7, 0.8), database, ids, audio_dict, AV_scores, model, device)\n",
    "\n",
    "with open('audio_projections.pkl','wb') as f:\n",
    "    pickle.dump(database, f)\n",
    "\n",
    "with open('audio_dict.pkl','wb') as f:\n",
    "    pickle.dump(audio_dict, f)\n",
    "\n",
    "with open('ids.pkl', 'wb') as f:\n",
    "    pickle.dump(ids, f)\n",
    "\n",
    "torch.save(model.state_dict(), 'model_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbd0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement updated model for survey (and send out)\n",
    "# TODO: Write standalone web app for model\n",
    "# TODO: Diss writing + other deliverables\n",
    "# TODO: Tidy up code and fully understand"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
